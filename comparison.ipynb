{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 질문으로 서로 다른 모델의 답변 비교해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/brianjang7/home1/.venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda112.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 112\n",
      "CUDA SETUP: Loading binary /home/brianjang7/home1/.venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda112.so...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer, pipeline\n",
    "from peft import PeftModel, PeftConfig, prepare_model_for_kbit_training\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "\n",
    "def load_model(model_id):\n",
    "    \n",
    "    print(model_id)\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(    # 모델의 성능을 유지하면서 메모리 사용을 최적화하고, 하드웨어 환경에 맞게 데이터를 처리하는 데 도움\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Set the available GPU devices\n",
    "    # available_gpus = [0]    # GPU의 개수가 1개 일때\n",
    "    available_gpus = [0, 1]  # GPU의 개수가 2개 일때\n",
    "    # available_gpus = [0,1,2,3]    # GPU의 개수가 4개 일때\n",
    "\n",
    "    # Function to find the GPU with the least memory usage\n",
    "    def get_least_memory_gpu():\n",
    "        least_memory = float('inf')\n",
    "        least_memory_gpu = None\n",
    "        for gpu in available_gpus:\n",
    "            allocated_memory = torch.cuda.memory_allocated(gpu)\n",
    "            if allocated_memory < least_memory:\n",
    "                least_memory = allocated_memory\n",
    "                least_memory_gpu = gpu\n",
    "        return least_memory_gpu\n",
    "\n",
    "    # Allocate the remaining GPU memory to the current process\n",
    "    device = get_least_memory_gpu()\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "    # Load the model and allocate GPU memory\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=device)\n",
    "\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    # 모델의 훈련 과정에서 필요한 메모리 양을 줄일 수 있습니다.\n",
    "    if model_id !=  'beomi/KoAlpaca-KoRWKV-6B':\n",
    "        model.gradient_checkpointing_enable()           # Activates gradient checkpointing for the current model.\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "            \n",
    "    model.eval()\n",
    "    model.config.use_cache = True  # silence the warnings. Please re-enable for inference!\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def prompt_formating(model_id,user_input):\n",
    "    \n",
    "    # llama-2의 프롬프트 형식\n",
    "    \"\"\"<s>[INST] <<SYS>>\n",
    "    {{ system_prompt }}\n",
    "    <</SYS>>\n",
    "\n",
    "    {{ user_msg_1 }} [/INST] {{ model_answer_1 }} </s>\\\n",
    "    <s>[INST] {{ user_msg_2 }} [/INST] {{ model_answer_2 }} </s>\\\n",
    "    <s>[INST] {{ user_msg_3 }} [/INST]\"\"\"\n",
    "    \n",
    "    # DEFAULT_SYSTEM_PROMPT \n",
    "    # You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "    # If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "    \n",
    "    \n",
    "    if model_id == \"beomi/llama-2-ko-7b\" or model_id == 'beomi/KoAlpaca-KoRWKV-6B' or model_id == 'kfkas/Llama-2-ko-7b-Chat' :     # beomi FORMAT\n",
    "        prompt = f\"### 질문: {user_input}\\n\\n### 답변:\"         \n",
    "        \n",
    "    elif model_id == 'nlpai-lab/kullm-polyglot-5.8b-v2':        # kullm-polyglot-5.8b-v2 FORMAT\n",
    "        prompt = f\"\"\"프롬프트 확인 : 아래는 작업을 설명하는 명령어와 추가 컨텍스트를 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 응답을 작성하세요.\n",
    "\n",
    "### 명령어:\n",
    "{user_input}\n",
    "\n",
    "### 입력:\n",
    "\n",
    "\n",
    "### 응답:\"\"\"\n",
    "        \n",
    "        \n",
    "    else:       # 기본 FORMAT\n",
    "        prompt = f\"### instruction: {user_input}\\n\\n### Response:\\n\"            \n",
    "        # prompt = f'### 질문: {user_input}\\n\\n### 답변:'      # beomi/KoAlpaca-KoRWKV-6B and \"beomi/polyglot-ko-12.8b-safetensors FORMAT        \n",
    "    \n",
    "    return prompt\n",
    "    \n",
    "def gen(model, model_id, tokenizer, user_input, max_new_tokens=256):     # gen 의 형식은 모델마다 다를 수 있으니 참고하여 프롬프트를 설정해주세요.\n",
    "\n",
    "    if model_id ==  'beomi/KoAlpaca-KoRWKV-6B':\n",
    "        eos_token_id = 0\n",
    "    else:\n",
    "        eos_token_id = 2\n",
    "    \n",
    "    prompt = prompt_formating(model_id,user_input)\n",
    "        \n",
    "    gened = model.generate(\n",
    "        **tokenizer(\n",
    "            prompt,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=False\n",
    "        ).to('cuda'),\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        early_stopping=True,\n",
    "        do_sample=False,                 # False : grid 샘플링\n",
    "        eos_token_id=eos_token_id,\n",
    "        pad_token_id=eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(gened[0])\n",
    "\n",
    "    \n",
    "# def generate_response(model, tokenizer, input_text):\n",
    "#     inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "#     outputs = model.generate(inputs, max_length=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MODEL = \"nlpai-lab/kullm-polyglot-5.8b-v2\"\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class Prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        if not template_name:\n",
    "            # Enforce the default here, so the constructor can be called with '' and will not break.\n",
    "            template_name = \"alpaca\"\n",
    "        file_name = osp.join(f\"{template_name}.json\")\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name) as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(\n",
    "                instruction=instruction\n",
    "            )\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        print(f'프롬프트 확인 : {res}')\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79de175e75b44d695ac5d7ee70dad59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL = \"nlpai-lab/kullm-polyglot-5.8b-v2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(device=1, non_blocking=True)\n",
    "model.eval()\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=MODEL, device=1)\n",
    "\n",
    "prompter = Prompter(\"kullm\")\n",
    "\n",
    "\n",
    "def infer(instruction=\"\", input_text=\"\"):\n",
    "    prompt = prompter.generate_prompt(instruction, input_text)\n",
    "    output = pipe(prompt, max_length=512, temperature=0.2, num_beams=5, eos_token_id=2)\n",
    "    s = output[0][\"generated_text\"]\n",
    "    result = prompter.get_response(s)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주석 풀면서 load하고 싶은 모델 load 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlpai-lab/kullm-polyglot-5.8b-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0918e68808417eb7db845c23aed0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# KoRWKV, KoRWKV_tokenizer = load_model('beomi/KoAlpaca-KoRWKV-6B')\n",
    "# Llama_2_ko_7b, Llama_2_ko_7b_tokenizer = load_model(\"beomi/llama-2-ko-7b\")\n",
    "# Llama_2_ko_7b_Chat, Llama_2_ko_7b_Chat_tokenizer = load_model('kfkas/Llama-2-ko-7b-Chat')\n",
    "# polyglot_ko, polyglot_ko_tokenizer = load_model(\"EleutherAI/polyglot-ko-5.8b\")\n",
    "polyglot_ko_kullm, polyglot_ko_kullm_tokenizer = load_model('nlpai-lab/kullm-polyglot-5.8b-v2')\n",
    "\n",
    "# KoRWKV_config = {\n",
    "#     'model' : KoRWKV,\n",
    "#     'tokenizer' : KoRWKV_tokenizer,\n",
    "#     'model_id' : 'beomi/KoAlpaca-KoRWKV-6B'\n",
    "# }\n",
    "\n",
    "# Llama_2_ko_7b_config = {\n",
    "#     'model' : Llama_2_ko_7b,\n",
    "#     'tokenizer' : Llama_2_ko_7b_tokenizer,\n",
    "#     'model_id' : \"beomi/llama-2-ko-7b\"\n",
    "# }\n",
    "\n",
    "# Llama_2_ko_7b_Chat_config = {\n",
    "#     'model' : Llama_2_ko_7b_Chat,\n",
    "#     'tokenizer' : Llama_2_ko_7b_Chat_tokenizer,\n",
    "#     'model_id' : 'kfkas/Llama-2-ko-7b-Chat'\n",
    "# }\n",
    "\n",
    "\n",
    "# polyglot_ko_config = {\n",
    "#     'model' : polyglot_ko,\n",
    "#     'tokenizer' : polyglot_ko_tokenizer,\n",
    "#     'model_id' : \"EleutherAI/polyglot-ko-5.8b\"\n",
    "# }\n",
    "\n",
    "\n",
    "polyglot_ko_kullm_config = {\n",
    "    'model' : polyglot_ko_kullm,\n",
    "    'tokenizer' : polyglot_ko_kullm_tokenizer,\n",
    "    'model_id' : 'nlpai-lab/kullm-polyglot-5.8b-v2'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/brianjang7/home1/.venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda112.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 112\n",
      "CUDA SETUP: Loading binary /home/brianjang7/home1/.venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda112.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b795baf9e44966bbcc2347fcd23692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"MODEL = 'beomi/KoAlpaca-Polyglot-5.8B'\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM\n",
    "\n",
    "MODEL = 'beomi/KoAlpaca-Polyglot-5.8B'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(device=f\"cuda\", non_blocking=True)\n",
    "model.eval()\n",
    "\n",
    "KoAlpaca_pipe = pipeline(\n",
    "    'text-generation', \n",
    "    model=model,\n",
    "    tokenizer=MODEL,\n",
    "    device=0\n",
    ")\n",
    "def ask(x, context='', is_input_full=False):\n",
    "    ans = KoAlpaca_pipe(\n",
    "        f\"### 질문: {x}\\n\\n### 맥락: {context}\\n\\n### 답변:\" if context else f\"### 질문: {x}\\n\\n### 답변:\", \n",
    "        do_sample=True, \n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        return_full_text=True,\n",
    "        eos_token_id=2,\n",
    "    )\n",
    "    print(ans[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 질문: 캠핑 여행에 필요한 10가지 품목의 목록\n",
      "\n",
      "### 답변:1. 천막\n",
      "                    \n",
      "2. 타프\n",
      "                    \n",
      "3. 테이블\n",
      "                    \n",
      "4. 체어\n",
      "                    \n",
      "5. 부츠\n",
      "                    \n",
      "6. 바인딩\n",
      "                    \n",
      "7. 스페츠\n",
      "                    \n",
      "8. 플렉스\n",
      "                    \n",
      "9.angle\n",
      "                    \n",
      "10.보조배터리\n",
      "                    \n",
      "\n",
      "이외에도, 랜턴, 급식용 테이블, 철모, 여분의 의류 등이 필요합니다.\n"
     ]
    }
   ],
   "source": [
    "ask(\"캠핑 여행에 필요한 10가지 품목의 목록\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 주석 풀었던 모델들을 마찬가지로 아래 코드에서도 주석 풀어주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "< nlpai-lab/kullm-polyglot-5.8b-v2 >\n",
      "프롬프트 확인 : 아래는 작업을 설명하는 명령어와 추가 컨텍스트를 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### 명령어:\n",
      "캠핑 여행에 필요한 10가지 품목의 목록\n",
      "\n",
      "### 입력:\n",
      "\n",
      "\n",
      "### 응답:\n",
      "캠핑 여행에 필요한 10가지 품목은 다음과 같습니다:\n",
      "\n",
      "1. 텐트: 텐트는 캠핑 여행의 필수 요소입니다. 텐트는 비바람을 막아주고 편안한 잠자리를 제공하며, 캠핑장에서의 하룻밤을 위한 아늑한 공간을 제공합니다.\n",
      "\n",
      "2. 침낭: 침낭은 따뜻하고 편안한 잠자리를 제공하는 필수 아이템입니다. 캠핑장에서는 담요나 담요를 제공하지 않는 경우가 많으므로 침낭이 필요합니다.\n",
      "\n",
      "3. 캠핑 의자: 캠핑 의자에 앉아 휴식을 취하거나 식사를 하는 것은 캠핑 여행의 또 다른 즐거움입니다. 캠핑 의자가 없으면 캠핑장에서 식사를 하거나 휴식을 취하기가 어려울 수 있습니다.\n",
      "\n",
      "4. 스토브: 스토브는 캠핑 여행의 필수품입니다. 스토브는 요리를 하고, 난방을 하고, 불을 피우는 데 사용할 수 있습니다.\n",
      "\n",
      "5. 조리 도구: 캠핑 여행에는 요리를 하는 데 필요한 도구가 필요합니다. 캠핑용 칼, 도마, 식기 등이 포함됩니다.\n",
      "\n",
      "6. 조리 도구: 캠핑 여행에는 요리를 하는 데 필요한 도구가 필요합니다. 캠핑용 칼,\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "\n",
    "model_ids = [polyglot_ko_kullm_config['model_id']]\n",
    "\n",
    "user_input = '캠핑 여행에 필요한 10가지 품목의 목록'   # 캠핑 여행에 필요한 10가지 품목의 목록을 생성합니다. /kfkas/Llama-2-ko-7b-Chat\n",
    "\n",
    "if user_input:\n",
    "    # KoRWKV_6B_answer = KoRWKV_gen(user_input)\n",
    "    # KoAlpaca_answer = ask(user_input)\n",
    "    # Llama_2_ko_7b_answer = gen(model=Llama_2_ko_7b_config['model'], model_id=Llama_2_ko_7b_config['model_id'], tokenizer=Llama_2_ko_7b_config['tokenizer'], user_input=user_input)\n",
    "    # Llama_2_ko_7b_Chat_answer = gen(model=Llama_2_ko_7b_Chat_config['model'], model_id=Llama_2_ko_7b_Chat_config['model_id'], tokenizer=Llama_2_ko_7b_Chat_config['tokenizer'], user_input=user_input)\n",
    "    # polyglot_ko_answer = gen(model=polyglot_ko_config['model'], model_id=polyglot_ko_config['model_id'], tokenizer=polyglot_ko_config['tokenizer'], user_input=user_input)\n",
    "    polyglot_ko_kullm_answer = gen(model=polyglot_ko_kullm_config['model'], model_id=polyglot_ko_kullm_config['model_id'], tokenizer=polyglot_ko_kullm_config['tokenizer'], user_input=user_input)\n",
    "    # polyglot_ko_kullm_answer_2 = infer(input_text=user_input)\n",
    "    \n",
    "    # responses.append(KoAlpaca_answer)\n",
    "    # responses.append(Llama_2_ko_7b_answer)\n",
    "    # responses.append(Llama_2_ko_7b_Chat_answer)\n",
    "    # responses.append(polyglot_ko_answer)\n",
    "    responses.append(polyglot_ko_kullm_answer)\n",
    "    # responses.append(polyglot_ko_kullm_answer_2)\n",
    "    \n",
    "        \n",
    "    # 답변들을 출력\n",
    "    for id, response in zip(model_ids,responses):\n",
    "        print()\n",
    "        print(f'< {id} >')\n",
    "        print(f\"{response}\")\n",
    "        print('------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 모델에서 학습한 내용에 대해서는 답변이 준수하게 나옴.\n",
    "그 중 가장 준수한 모델은 구름 모델.<br>\n",
    "\n",
    "괜찮은 모델 : \n",
    "1. **beomi/KoAlpaca-Polyglot-5.8B** Polyglot 모델을 KoAlpaca Datasets(21k)로 학습한 모델 <br>\n",
    "2. **nlpai-lab/kullm-polyglot-5.8b-v2** Polyglot 모델을 **GPT4ALL, Dolly, Vicuna 데이터셋(153k, nlpai-lab/kullm-v2)으로 학습한 모델, 8에폭 진행,  Low Rank Adaptation (LoRA) <br>\n",
    "\n",
    "개인적인 성능 평가 결과 : **beomi/KoAlpaca-Polyglot-5.8B < nlpai-lab/kullm-polyglot-5.8b-v2**   (둘다 성능은 괜찮으나 nlpai-lab에서 만든 모델이 좀더 좋다.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
